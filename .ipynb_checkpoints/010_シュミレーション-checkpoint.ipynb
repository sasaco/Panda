{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import sqlite3\nimport datetime\nimport collections\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport random\nimport csv\nimport os\n\nclass QLearningDecisionPolicy():\n\n    def __init__(self, actions, input_dim):\n    \n        # 学習率\n        self.gamma = 0.001\n        # 行動の種類\n        self.actions = actions\n        # 出力層の数 = 行動数\n        output_dim = len(actions)\n        # 中間層の数\n        h1_dim = 300\n\n        self.x = tf.placeholder(tf.float32, [None, input_dim])\n        self.y = tf.placeholder(tf.float32, [output_dim])\n\n        W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))\n        b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n        h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n\n        W2 = tf.Variable(tf.random_normal([h1_dim, h1_dim]))\n        b2 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n        h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n\n        W3 = tf.Variable(tf.random_normal([h1_dim, h1_dim]))\n        b3 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n        h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\n\n        W4 = tf.Variable(tf.random_normal([h1_dim, output_dim]))\n        b4 = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n        h4 = tf.nn.relu(tf.matmul(h3, W4) + b4)\n\n        self.q = h4\n        self.loss = tf.square(self.y - self.q)\n        self.train_op = tf.train.AdagradOptimizer(0.01).minimize(self.loss)\n        self.sess = tf.Session()\n        self.sess.run(tf.initialize_all_variables())\n\n        # ランダムに行動を決定する確率\n        self.eps = 0.9\n\n        # 報酬計算用\n        self.budget = 0\n\n        # モデル保存用\n        self.saver = tf.train.Saver()\n        self.model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"models\")\n        self.model_name = \"{}.ckpt\".format(self.__class__.__name__)\n\n    def select_action(self, current_state, step):\n\n        # ランダムで手を選択する確立\n        threshold = min(self.eps, step / 1000.)\n\n        if random.random() < threshold:\n            # Q 値のもっとも大きい行動を選択する\n            action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n            action_idx = np.argmax(action_q_vals)  # TODO: replace w/ tensorflow's argmax\n            action = self.actions[action_idx]\n        else:\n            # ランダムに手を選択する\n            action = self.actions[random.randint(0, len(self.actions) - 1)]\n\n        return action\n\n    # def update_q(self, state, action, reward, next_state):\n    def update_q(self, transitions, current_portfolio):\n\n        # 売った時の利益\n        reward = (current_portfolio - self.budget) / len(transitions)\n        \n        for state, action, next_state in transitions:\n\n            # 今の状況のときにとる行動のQ値\n            action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n            # 次の状況のときにとる行動のQ値\n            next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n            # 次の状況のときにとる行動のうちもっとも大きいQ値を有する行動\n            next_action_idx = np.argmax(next_action_q_vals)\n            # 今の状況のときにとる行動のQ値を更新\n            action_q_vals[0, next_action_idx] = reward + self.gamma * next_action_q_vals[0, next_action_idx]\n            # いらない次元を無くす\n            action_q_vals = np.squeeze(np.asarray(action_q_vals))\n            # 学習\n            self.sess.run(self.train_op, feed_dict={self.x: state, self.y: action_q_vals})\n\n        self.budget = current_portfolio\n\n\n    def load_model(self, model_path=None):\n        if model_path:\n            # load from model_path\n            self.saver.restore(self.sess, model_path)\n        else:\n            # load from checkpoint\n            checkpoint = tf.train.get_checkpoint_state(self.model_dir)\n            if checkpoint and checkpoint.model_checkpoint_path:\n                self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n\n\n    def save_model(self):\n        self.saver.save(self.sess, os.path.join(self.model_dir, self.model_name))\n\n\n\ndef run_simulation(policy, initial_budget, initial_num_stocks, stock_prices, hist, debug=False):\n\n    budget = initial_budget\n    num_stocks = initial_num_stocks\n    share_value = 0\n    transitions = []\n\n    # クボタをシュミレーション対象にする\n    target = stock_prices['t6326']\n    count = len(target) - hist - 1\n\n    next_state = None\n    next_prices = []\n\n    for i in range(len(target) - hist - 1):\n\n        if len(next_prices) > 0:\n            prices = next_prices\n            current_state = next_state\n        else:\n            # 学習期間 * 株価情報銘柄数 * len(Oepn, High, Low, Close, Volume) のテーブルを作成\n            prices = []\n            for symbol in stock_prices:\n                current = stock_prices[symbol]\n                for j in range(i, i+hist):\n                    Open = current[j][2]\n                    High = current[j][3]\n                    Low =  current[j][4]\n                    Close = current[j][5]\n                    Volume = current[j][6]\n                    prices.extend([Open, High, Low, Close, Volume])\n\n            # 現在の盤面\n            current_state = np.asmatrix(np.hstack((prices, budget, num_stocks)))\n\n        # 行動する前の 資金\n        current_portfolio = budget + num_stocks * share_value\n\n        # for log\n        if i % 100 == 0:\n            print('progress {:.2f}%, portfolio {}'.format(float(100*i) / (len(target) - hist - 1), current_portfolio ))\n\n        # 行動を選択する\n        action = policy.select_action(current_state, i)\n\n        # 今日の株価\n        current = target[i + hist + 1]\n        buy_value = float(current[3])   # 買う時は高く買わされたことにする\n        sell_value = float(current[4])  # 売る時は安く売ってしまったことにする\n        share_value = float(current[5]) # 資産計算用価格は終値\n\n        # 行動を実行する\n        if action == 'Buy' and budget >= buy_value*100:\n            # 100株単位で売買する\n            budget -= buy_value*100\n            num_stocks += 100 \n\n        elif action == 'Sell' and num_stocks > 0:\n            # 100株単位で売買する\n            budget += sell_value*100\n            num_stocks -= 100 \n\n        else:\n            action = 'Hold'\n\n\n        # 行動をした後の 資金\n        new_portfolio = budget + num_stocks * share_value\n\n        # 行動を反映した前後の資金の増加分を報酬とする\n        reward = new_portfolio - current_portfolio\n\n\n        # 次の日の状態を生成\n        # 学習期間 * 株価情報銘柄数 * len(Oepn, High, Low, Close, Volume) のテーブルを作成\n        next_prices = []\n        for symbol in stock_prices:\n            current = stock_prices[symbol]\n            for j in range(i+1, i+hist+1):\n                Open = current[j][2]\n                High = current[j][3]\n                Low =  current[j][4]\n                Close = current[j][5]\n                Volume = current[j][6]\n                next_prices.extend([Open, High, Low, Close, Volume])\n\n        next_state = np.asmatrix(np.hstack((next_prices, budget, num_stocks)))\n \n        # 学習用データを取っておく\n        if action == 'Sell' or num_stocks > 0:\n            transitions.append([current_state, action, next_state])\n        else:\n            transitions = []\n\n        # 学習\n        if action == 'Sell':\n            policy.update_q(transitions, new_portfolio)\n            # policy.update_q(current_state, action, reward, next_state)\n\n\n    # 最終的な資産\n    portfolio = budget + num_stocks * share_value\n\n    if debug:\n        print('${}\\t{} shares'.format(budget, num_stocks))\n\n    return portfolio\n\n\ndef run_simulations(policy, budget, num_stocks, stock_prices, hist):\n    \n    # 試行回数\n    num_tries = 10\n\n    # 試行結果の格納配列\n    final_portfolios = list()\n    \n    for i in range(num_tries):\n        # 試行開始\n        final_portfolio = run_simulation(policy, budget, num_stocks, stock_prices, hist)\n        # 試行結果の格納\n        final_portfolios.append(final_portfolio)\n\n    # 試行結果の配列の平均\n    avg = np.mean(final_portfolios) \n    # 試行結果の配列の標準偏差\n    std = np.std(final_portfolios)  \n    \n    return avg, std\n\n\ndef get_prices(start_date, end_date):\n    \n    stock_prices = collections.OrderedDict()\n\n    baseDate = datetime.datetime.strptime('1983/1/1', \"%Y/%m/%d\")\n    s = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n    e = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n    start_id = (s  - baseDate).days\n    end_id = (e - baseDate).days\n\n    # データベースファイルのパス\n    dbpath = 'tpix100db.sqlite'\n\n    # データベース接続とカーソル生成\n    connection = sqlite3.connect(dbpath)\n    cursor = connection.cursor()\n\n    # データベースに保存されているすべての銘柄を取得\n    sql = \"select name from sqlite_master where type='table'\"\n    cursor.execute(sql)\n    codes = cursor.fetchall()\n\n    # すべての銘柄の対象期間の株価情報を取得\n    count = 0\n    for code in codes:\n\n        symbol = code[0]\n\n        # start日と end日がない銘柄は除外する\n        sql = 'select COUNT(*) from {} where id = {}'.format(symbol, start_id)\n        cursor.execute(sql)\n        res = cursor.fetchall()\n        start_count = res[0][0]\n\n        sql = 'select COUNT(*) from {} where id = {}'.format(symbol, end_id)\n        cursor.execute(sql)\n        res = cursor.fetchall()\n        end_count = res[0][0]\n\n        if start_count + end_count < 2:\n            continue\n\n        # 対象期間の株価情報を取得\n        sql = 'SELECT * FROM {} WHERE id BETWEEN {} AND {}'.format(symbol, start_id, end_id)\n        cursor.execute(sql)\n        res = cursor.fetchall()\n\n        # 最初の銘柄と数が合わないものは除外する\n        if count == 0:\n            count = len(res)\n        elif count <= len(res):\n            stock_prices[symbol] = res\n\n    # 接続を閉じる\n    connection.close()\n\n    # 答えを返す\n    return stock_prices\n\n\ndef plot_prices(prices: list):\n\n    plt.title('Opening stock prices')\n    plt.xlabel('day')\n    plt.ylabel('price ($)')\n    plt.plot(prices)\n    plt.savefig('prices.png')\n\n\nif __name__ == '__main__':\n\n    # 対象期間のすべての銘柄の株価情報を取得\n    # stock_prices = get_prices('1992-07-22', '2016-07-22')\n    stock_prices = get_prices('1988-10-01', '2001-07-30')\n    num_prices = len(stock_prices)\n\n    # 学習期間\n    hist = 200\n\n    # クボタだけ取り出す\n    target_prices = stock_prices['t6326']\n    plot_prices([row[5] for row in target_prices]) # 終値のグラフを描く\n\n    # 行動の種類\n    actions = ['Buy', 'Sell', 'Hold']\n\n    # 入力数 = 学習期間 * 株価情報銘柄数 * len(Oepn, High, Low, Close, Volume) + 資金 + 保有株式数\n    input_dim = (hist * num_prices * 5) + 2\n    # ニュートラルネットワークの初期化\n    policy = QLearningDecisionPolicy(actions, input_dim)\n    # 保存してあるものがあれば読み込む\n    policy.load_model()\n\n    avg = 0\n    while avg < 20000000.0: # 資産が2倍以上になるまでくりかえす\n\n        # 資金\n        budget = 10000000.0 \n        policy.budget = budget\n\n        # 保有株式数\n        num_stocks = 0 \n\n        # シュミレーション開始\n        avg, std = run_simulations(policy, budget, num_stocks, stock_prices, hist)\n\n        # 燃えるを保存\n        policy.save_model()\n\n    print(\"学習後の資産の平均{}, 学習後の資産の標準偏差{}\".format(avg, std))\n\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/ x - python",
      "nbconvert_exporter": "python",
      "version": "3.6.0",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython2",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}